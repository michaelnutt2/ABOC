{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ABOC GPU Benchmark on Google Colab\n",
                "\n",
                "This notebook runs the Encoder and Parallel Decoder on a GPU, using **pre-generated FlatBuffer data** from Google Cloud Storage (GCS).\n",
                "\n",
                "## Prerequisites\n",
                "1.  **Google Drive**: Upload your entire `ABOC` project folder to your Google Drive.\n",
                "2.  **GCS Bucket**: Ensure you have access to `gs://mtn_fb_file_bucket`.\n",
                "3.  **GPU Runtime**: Change Runtime Type to **T4 GPU** (or better)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Mount Google Drive (to access the code)\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Setup Environment\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# Change this path to where you uploaded the ABOC folder\n",
                "PROJECT_PATH = '/content/drive/MyDrive/ABOC' \n",
                "if not os.path.exists(PROJECT_PATH):\n",
                "    print(f\"ERROR: Path {PROJECT_PATH} does not exist. Please check your Drive structure.\")\n",
                "else:\n",
                "    os.chdir(PROJECT_PATH)\n",
                "    sys.path.append(PROJECT_PATH)\n",
                "    print(f\"Working Directory set to: {os.getcwd()}\")\n",
                "\n",
                "# Install dependencies\n",
                "# Added hdf5storage to fix ImportErrors in legacy modules\n",
                "# Pinning numpy<2 to avoid PyTorch binary incompatibility (ValueError: METH_CLASS)\n",
                "!pip install \"numpy<2\" flatbuffers plyfile Ninja hdf5storage\n",
                "!sudo apt-get install ninja-build"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Authenticate with Google Cloud (for GCS Access)\n",
                "from google.colab import auth\n",
                "auth.authenticate_user()\n",
                "\n",
                "# Set project ID if needed (optional for public/accessible buckets)\n",
                "# !gcloud config set project YOUR_PROJECT_ID"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. JIT Compile the C++ Backend (Force Rebuild to ensure GPU/Colab compat)\n",
                "import torch\n",
                "from torch.utils.cpp_extension import load\n",
                "\n",
                "print(\"Compiling numpyAc C++ backend...\")\n",
                "try:\n",
                "    # Remove old build artifacts if they exist to force clean build\n",
                "    !rm -rf ./numpyAc/backend/build\n",
                "    \n",
                "    # Load/Compile\n",
                "    # This will compile numpyAc_backend.cpp and bind it\n",
                "    import numpyAc.numpyAc \n",
                "    print(\"Success: numpyAc module loaded.\")\n",
                "except Exception as e:\n",
                "    print(f\"Compilation Error: {e}\")\n",
                "    # Fallback compilation command if import fails (debug)\n",
                "    # !cd numpyAc/backend && python3 setup.py install"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Configuration (Loaded from config.py)\n",
                "import config\n",
                "\n",
                "BUCKET_NAME = config.BUCKET_NAME\n",
                "FB_PREFIX = config.GCS_DATA_PREFIX\n",
                "CKPT_PREFIX = config.GCS_CHECKPOINT_PREFIX\n",
                "\n",
                "# Sampling Configuration\n",
                "# Statistical Significance Guide (Pop = 48,000):\n",
                "# - 381 samples: 95% Confidence, 5% Margin of Error (Gold Standard)\n",
                "# - 100 samples: 95% Confidence, ~10% Margin of Error (Good Engineering Trade-off)\n",
                "# - 50 samples: Quick Smoke Test\n",
                "NUM_SAMPLES = 50\n",
                "\n",
                "TEMP_DIR = \"/content/tmp_data\"\n",
                "os.makedirs(TEMP_DIR, exist_ok=True)\n",
                "os.makedirs(f\"{TEMP_DIR}/decoded\", exist_ok=True)\n",
                "\n",
                "import tensorflow as tf\n",
                "import glob\n",
                "print(f\"TensorFlow Device: {tf.config.list_physical_devices('GPU')}\")\n",
                "print(f\"Using Bucket: {BUCKET_NAME}\")\n",
                "\n",
                "# -----------------------------------------------------\n",
                "# Fetch Best Checkpoint from GCS\n",
                "# -----------------------------------------------------\n",
                "print(f\"Listing checkpoints in gs://{BUCKET_NAME}/{CKPT_PREFIX}...\")\n",
                "gcs_ckpts = !gsutil ls gs://{BUCKET_NAME}/{CKPT_PREFIX}/*.weights.h5\n",
                "ckpt_candidates = [c for c in gcs_ckpts if c.endswith('.weights.h5')]\n",
                "\n",
                "MODEL_PATH = None\n",
                "\n",
                "if ckpt_candidates:\n",
                "    ckpt_candidates.sort()\n",
                "    best_ckpt_gcs = ckpt_candidates[-1]\n",
                "    ckpt_filename = os.path.basename(best_ckpt_gcs)\n",
                "    MODEL_PATH = f\"{TEMP_DIR}/{ckpt_filename}\"\n",
                "    \n",
                "    print(f\"Downloading checkpoint: {best_ckpt_gcs} -> {MODEL_PATH}\")\n",
                "    !gsutil cp {best_ckpt_gcs} {MODEL_PATH}\n",
                "else:\n",
                "    print(\"WARNING: No checkpoints found in GCS! Falling back to Drive default if available.\")\n",
                "    default_drive_path = os.path.join(PROJECT_PATH, 'modelsave/lidar/checkpoints_model_epoch_50.weights.h5')\n",
                "    if os.path.exists(default_drive_path):\n",
                "        MODEL_PATH = default_drive_path\n",
                "        print(f\"Using Drive Checkpoint: {MODEL_PATH}\")\n",
                "    else:\n",
                "        print(\"ERROR: No checkpoints found anywhere. Inference will use Random Weights.\")\n",
                "\n",
                "# -----------------------------------------------------\n",
                "\n",
                "from encoder_tf import EncoderTF\n",
                "from decoder_tf_parallel import DecoderTFParallel\n",
                "\n",
                "# Initialize Models (Loads Weights from Local Temp Path)\n",
                "if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
                "    encoder = EncoderTF(f\"{TEMP_DIR}/encoder_log.txt\", model_path=MODEL_PATH)\n",
                "    decoder = DecoderTFParallel(model_path=MODEL_PATH)\n",
                "else:\n",
                "    print(\"Initializing with RANDOM WEIGHTS (Benchmark is valid for speed, invalid for BPP).\")\n",
                "    encoder = EncoderTF(f\"{TEMP_DIR}/encoder_log.txt\")\n",
                "    decoder = DecoderTFParallel()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Benchmark Loop (With Averaging & Accuracy Check)\n",
                "import time\n",
                "import subprocess\n",
                "import glob\n",
                "import random\n",
                "import shutil\n",
                "import numpy as np\n",
                "import OctreeData.Dataset as Dataset\n",
                "\n",
                "def count_points_in_fb(fb_path):\n",
                "    try:\n",
                "        with open(fb_path, 'rb') as f:\n",
                "            buf = f.read()\n",
                "            dataset = Dataset.Dataset.GetRootAsDataset(buf, 0)\n",
                "            # Total Octree Nodes as input complexity proxy\n",
                "            return dataset.NodesLength()\n",
                "    except Exception:\n",
                "        return 0\n",
                "\n",
                "def count_points_in_ply(ply_path):\n",
                "    try:\n",
                "        with open(ply_path, 'r') as f:\n",
                "            header = True\n",
                "            count = 0\n",
                "            for line in f:\n",
                "                if header:\n",
                "                    if line.startswith('element vertex'):\n",
                "                        return int(line.split()[-1])\n",
                "                    if line.strip() == 'end_header':\n",
                "                        header = False\n",
                "        return 0\n",
                "    except:\n",
                "        return 0\n",
                "\n",
                "# Metrics Storage\n",
                "stats = {\n",
                "    'download': [],\n",
                "    'encode': [],\n",
                "    'decode': [],\n",
                "    'bpp': [], \n",
                "    'input_nodes': [],\n",
                "    'output_points': []\n",
                "}\n",
                "\n",
                "def benchmark_gcs_file(gcs_path):\n",
                "    filename = os.path.basename(gcs_path)\n",
                "    local_fb = f\"{TEMP_DIR}/{filename}\"\n",
                "    local_encoded = f\"{TEMP_DIR}/{filename}.enc.bin\"\n",
                "    local_decoded = f\"{TEMP_DIR}/decoded/{filename}.ply\"\n",
                "    \n",
                "    print(f\"--- Processing {filename} ---\")\n",
                "    \n",
                "    # A. Download (.fb)\n",
                "    t0 = time.time()\n",
                "    !gsutil cp {gcs_path} {local_fb}\n",
                "    download_time = time.time() - t0\n",
                "    \n",
                "    # Get Input Stats\n",
                "    input_node_count = count_points_in_fb(local_fb)\n",
                "    \n",
                "    # B. Encode (GPU)\n",
                "    t1 = time.time()\n",
                "    # Note: EncoderTF.compress returns bpp=0.0 for FB inputs.\n",
                "    _, size_bytes = encoder.compress(local_fb)\n",
                "    \n",
                "    basename = os.path.splitext(filename)[0]\n",
                "    default_out = f\"./Exp/Kitti_TF/data/{basename}.bin\"\n",
                "    \n",
                "    if os.path.exists(default_out):\n",
                "        shutil.move(default_out, local_encoded)\n",
                "    else:\n",
                "        print(\"Error: Encoder output not found.\")\n",
                "        return\n",
                "        \n",
                "    enc_time = time.time() - t1\n",
                "    \n",
                "    # C. Decode (GPU Parallel)\n",
                "    t2 = time.time()\n",
                "    decoder.decode(local_encoded, local_decoded)\n",
                "    dec_time = time.time() - t2\n",
                "    \n",
                "    # Accuracy Check\n",
                "    out_pt_count = count_points_in_ply(local_decoded)\n",
                "    \n",
                "    # BPP Calculation (Bits per Output Point)\n",
                "    bpp_custom = (size_bytes * 8.0) / max(1, out_pt_count) \n",
                "    \n",
                "    print(f\"  DL: {download_time:.3f}s | Enc: {enc_time:.3f}s | Dec: {dec_time:.3f}s\")\n",
                "    print(f\"  Size: {size_bytes} B | BPP (est): {bpp_custom:.2f}\")\n",
                "    print(f\"  Accuracy Check: InputNodes={input_node_count}, OutputPoints={out_pt_count}\")\n",
                "    \n",
                "    # Store Stats\n",
                "    stats['download'].append(download_time)\n",
                "    stats['encode'].append(enc_time)\n",
                "    stats['decode'].append(dec_time)\n",
                "    stats['bpp'].append(bpp_custom)\n",
                "    stats['input_nodes'].append(input_node_count)\n",
                "    stats['output_points'].append(out_pt_count)\n",
                "    \n",
                "    # Cleanup\n",
                "    if os.path.exists(local_fb): os.remove(local_fb)\n",
                "    if os.path.exists(local_encoded): os.remove(local_encoded)\n",
                "    if os.path.exists(local_decoded): os.remove(local_decoded)\n",
                "\n",
                "# List files in Bucket (FlatBuffers)\n",
                "print(\"Listing .fb files in GCS...\")\n",
                "gcs_list = !gsutil ls gs://{BUCKET_NAME}/{FB_PREFIX}/*.fb\n",
                "all_files = [f for f in gcs_list if f.endswith('.fb')]\n",
                "print(f\"Found {len(all_files)} FlatBuffer files.\")\n",
                "\n",
                "# Sample N Files\n",
                "num_to_sample = min(len(all_files), NUM_SAMPLES)\n",
                "print(f\"Benchmarking {num_to_sample} random files...\")\n",
                "samples = random.sample(all_files, num_to_sample)\n",
                "\n",
                "for f in samples:\n",
                "    benchmark_gcs_file(f)\n",
                "\n",
                "# -----------------------------------------------------\n",
                "# FINAL SUMMARY\n",
                "# -----------------------------------------------------\n",
                "if len(stats['encode']) > 0:\n",
                "    avg_enc = sum(stats['encode']) / len(stats['encode'])\n",
                "    avg_dec = sum(stats['decode']) / len(stats['decode'])\n",
                "    avg_bpp = sum(stats['bpp']) / len(stats['bpp'])\n",
                "    \n",
                "    print(\"\\n==========================================\")\n",
                "    print(\"           BENCHMARK RESULTS              \")\n",
                "    print(\"==========================================\")\n",
                "    print(f\"Files Processed:   {len(stats['encode'])}\")\n",
                "    print(f\"Avg Encode Time:   {avg_enc:.4f} s  ({1.0/avg_enc:.2f} Hz)\")\n",
                "    print(f\"Avg Decode Time:   {avg_dec:.4f} s  ({1.0/avg_dec:.2f} Hz)\")\n",
                "    print(f\"Avg Compressed BPP:{avg_bpp:.4f} (Bits per Point)\")\n",
                "    print(\"==========================================\")\n",
                "    \n",
                "    # Note on Accuracy\n",
                "    print(\"Accuracy Validation (Sample 5):\")\n",
                "    for i, count in enumerate(stats['output_points'][:5]):\n",
                "        print(f\"  File {i+1}: {count} points reconstructed.\")\n",
                "else:\n",
                "    print(\"No files processed.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}