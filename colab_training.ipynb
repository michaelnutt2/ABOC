{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ABOC Model Training on Google Colab (TensorFlow + FlatBuffers)\n",
                "\n",
                "This notebook trains the Ancestor-Based Octree Compression (ABOC) model using TensorFlow on TPUs. \n",
                "It loads data from **FlatBuffer (.fb)** files stored in a Google Cloud Storage (GCS) bucket.\n",
                "\n",
                "**Key Features:**\n",
                "*   **TPU Acceleration:** Uses `tf.distribute.TPUStrategy` for high-performance training.\n",
                "*   **GCS Integration:** Streams data directly from GCS (`gs://mtn_fb_file_bucket`) to local Colab runtime for processing.\n",
                "*   **FlatBuffers:** Efficiently parses `.fb` files without external compilation steps (schema code is inlined).\n",
                "*   **Custom Data Pipeline:** Reconstructs octree context (Parent + Neighbors) on-the-fly."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "40fb4e9b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q tensorflow gcsfs flatbuffers\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import time\n",
                "import math\n",
                "import glob\n",
                "import tempfile\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "import flatbuffers\n",
                "import gcsfs\n",
                "\n",
                "# Authenticate with Google Cloud\n",
                "from google.colab import auth\n",
                "auth.authenticate_user()\n",
                "print(\"Authenticated with Google Cloud.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- GCS Configuration ---\n",
                "GCS_BUCKET = \"mtn_fb_file_bucket\"\n",
                "GCS_BASE_URI = f\"gs://{GCS_BUCKET}\"\n",
                "# Pattern to match .fb files. Adjust if files are in a subdir (e.g., 'data/*.fb')\n",
                "GCS_DATA_PATTERN = f\"{GCS_BASE_URI}/*.fb\"\n",
                "GCS_LOG_DIR = f\"{GCS_BASE_URI}/logs\"\n",
                "GCS_CHECKPOINT_DIR = f\"{GCS_BASE_URI}/checkpoints\"\n",
                "\n",
                "# --- Hyperparameters (Matching octAttention.py) ---\n",
                "EPOCHS = 50\n",
                "CONTEXT_LEN = 17 # Parent + 16 Neighbors\n",
                "MAX_OCTREE_LEVEL = 21 # Matching networkTool.py\n",
                "\n",
                "# Model Params\n",
                "VOCAB_SIZE = 256\n",
                "EMBED_DIM = 140 # 130 + 6 + 4\n",
                "NUM_HEADS = 4\n",
                "FF_DIM = 300\n",
                "NUM_LAYERS = 3\n",
                "DROPOUT = 0.0\n",
                "\n",
                "# Learning Rate\n",
                "INITIAL_LR = 1e-3\n",
                "\n",
                "# --- TPU Setup ---\n",
                "try:\n",
                "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
                "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
                "    tf.config.experimental_connect_to_cluster(tpu)\n",
                "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
                "    strategy = tf.distribute.TPUStrategy(tpu)\n",
                "except ValueError:\n",
                "    print('TPU not found. Using Default Strategy (CPU/GPU).')\n",
                "    strategy = tf.distribute.get_strategy()\n",
                "\n",
                "BATCH_SIZE_PER_REPLICA = 128 # Adjust based on memory\n",
                "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
                "print(f\"Global Batch Size: {GLOBAL_BATCH_SIZE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. FlatBuffers Definitions (Inlined)\n",
                "\n",
                "Inlining the Python code generated by `flatc` for `OctreeData.Dataset` and `OctreeData.OctreeNode` so this notebook is self-contained."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import flatbuffers\n",
                "from flatbuffers.compat import import_numpy\n",
                "np_fb = import_numpy()\n",
                "\n",
                "class OctreeNode(object):\n",
                "    __slots__ = ['_tab']\n",
                "    @classmethod\n",
                "    def GetRootAs(cls, buf, offset=0):\n",
                "        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)\n",
                "        x = OctreeNode()\n",
                "        x.Init(buf, n + offset)\n",
                "        return x\n",
                "    def Init(self, buf, pos):\n",
                "        self._tab = flatbuffers.table.Table(buf, pos)\n",
                "    def Level(self):\n",
                "        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))\n",
                "        if o != 0:\n",
                "            return self._tab.Get(flatbuffers.number_types.Int32Flags, o + self._tab.Pos)\n",
                "        return 0\n",
                "    def Octant(self):\n",
                "        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(12))\n",
                "        if o != 0:\n",
                "            return self._tab.Get(flatbuffers.number_types.Int32Flags, o + self._tab.Pos)\n",
                "        return 0\n",
                "    def Occupancy(self):\n",
                "        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(14))\n",
                "        if o != 0:\n",
                "            return self._tab.Get(flatbuffers.number_types.Int32Flags, o + self._tab.Pos)\n",
                "        return 0\n",
                "    def ParentOccupancy(self):\n",
                "        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(18))\n",
                "        if o != 0:\n",
                "            return self._tab.Get(flatbuffers.number_types.Int32Flags, o + self._tab.Pos)\n",
                "        return 0\n",
                "    def NeighborOccupancies(self, j):\n",
                "        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(20))\n",
                "        if o != 0:\n",
                "            a = self._tab.Vector(o)\n",
                "            return self._tab.Get(flatbuffers.number_types.Int32Flags, a + flatbuffers.number_types.UOffsetTFlags.py_type(j * 4))\n",
                "        return 0\n",
                "    def NeighborOccupanciesLength(self):\n",
                "        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(20))\n",
                "        if o != 0:\n",
                "            return self._tab.VectorLen(o)\n",
                "        return 0\n",
                "\n",
                "class Dataset(object):\n",
                "    __slots__ = ['_tab']\n",
                "    @classmethod\n",
                "    def GetRootAsDataset(cls, buf, offset=0):\n",
                "        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)\n",
                "        x = Dataset()\n",
                "        x.Init(buf, n + offset)\n",
                "        return x\n",
                "    def Init(self, buf, pos):\n",
                "        self._tab = flatbuffers.table.Table(buf, pos)\n",
                "    def Nodes(self, j):\n",
                "        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))\n",
                "        if o != 0:\n",
                "            x = self._tab.Vector(o)\n",
                "            x += flatbuffers.number_types.UOffsetTFlags.py_type(j) * 4\n",
                "            x = self._tab.Indirect(x)\n",
                "            obj = OctreeNode()\n",
                "            obj.Init(self._tab.Bytes, x)\n",
                "            return obj\n",
                "        return None\n",
                "    def NodesLength(self):\n",
                "        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))\n",
                "        if o != 0:\n",
                "            return self._tab.VectorLen(o)\n",
                "        return 0"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PrintLog:\n",
                "    def __init__(self, log_path):\n",
                "        self.log_path = log_path\n",
                "        self.fs = gcsfs.GCSFileSystem()\n",
                "        # Ensure dir exists\n",
                "        try:\n",
                "            self.fs.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
                "        except:\n",
                "            pass\n",
                "\n",
                "    def __call__(self, msg):\n",
                "        print(msg)\n",
                "        try:\n",
                "            with self.fs.open(self.log_path, 'ab') as f:\n",
                "                f.write((msg + \"\\n\").encode('utf-8'))\n",
                "        except Exception as e:\n",
                "            print(f\"Warning: Failed to write to GCS log: {e}\")\n",
                "\n",
                "print_log = PrintLog(f\"{GCS_LOG_DIR}/training_{int(time.time())}.log\")\n",
                "print_log(\"Starting training session.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Definition (TensorFlow/Keras Port)\n",
                "\n",
                "Ported from `octAttention.py` (PyTorch) to TensorFlow/Keras.\n",
                "Inputs: `[Batch, 17, 3]` (Channels: Occupancy, Level, Octant)\n",
                "Output: `[Batch, 256]` (Logits)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3009aad6",
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_model():\n",
                "    # Inputs: [Batch, Sequence=17, Channels=3]\n",
                "    # Channels: 0=Occupancy, 1=Level, 2=Octant\n",
                "\n",
                "    inputs = keras.Input(shape=(CONTEXT_LEN, 3), dtype=tf.int32)\n",
                "\n",
                "    # Split channels\n",
                "    occ_input = inputs[:, :, 0]\n",
                "    lvl_input = inputs[:, :, 1]\n",
                "    oct_input = inputs[:, :, 2]\n",
                "\n",
                "    # Embeddings (Matching octAttention.py)\n",
                "    # self.encoder = nn.Embedding(ntoken, 130)\n",
                "    occ_emb = layers.Embedding(VOCAB_SIZE, 130)(occ_input)\n",
                "\n",
                "    # self.encoder1 = nn.Embedding(MAX_OCTREE_LEVEL + 1, 6)\n",
                "    lvl_emb = layers.Embedding(MAX_OCTREE_LEVEL + 1, 6)(lvl_input)\n",
                "\n",
                "    # self.encoder2 = nn.Embedding(9, 4)\n",
                "    oct_emb = layers.Embedding(9, 4)(oct_input)\n",
                "\n",
                "    # Concatenate: 130 + 6 + 4 = 140\n",
                "    x = layers.Concatenate(axis=-1)([occ_emb, lvl_emb, oct_emb])\n",
                "\n",
                "    # Custom Layer to handle SparseTensor from TPU Embeddings\n",
                "    class ToDense(layers.Layer):\n",
                "        def __init__(self, **kwargs):\n",
                "            super(ToDense, self).__init__(**kwargs)\n",
                "\n",
                "        def compute_output_shape(self, input_shape):\n",
                "            return input_shape\n",
                "\n",
                "        def call(self, inputs):\n",
                "            # Check for SparseTensor (kinda redundant with Keras, but safe)\n",
                "            if isinstance(inputs, tf.SparseTensor):\n",
                "                return tf.sparse.to_dense(inputs)\n",
                "            # Check for KerasTensor's sparse property\n",
                "            if hasattr(inputs, 'sparse') and inputs.sparse:\n",
                "                return tf.sparse.to_dense(inputs)\n",
                "            return inputs\n",
                "\n",
                "    x = ToDense()(x)\n",
                "\n",
                "    # Scale by sqrt(embedding_dim) - PyTorch does this in forward\n",
                "    x = x * tf.math.sqrt(tf.cast(140.0, tf.float32))\n",
                "\n",
                "    # Positional Encoding\n",
                "    class PositionalEncoding(layers.Layer):\n",
                "        def __init__(self, d_model, max_len=5000):\n",
                "            super().__init__()\n",
                "            self.d_model = d_model\n",
                "            # Compute PE once\n",
                "            position = np.arange(max_len)[:, np.newaxis]\n",
                "            div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
                "            pe = np.zeros((max_len, d_model))\n",
                "            pe[:, 0::2] = np.sin(position * div_term)\n",
                "            pe[:, 1::2] = np.cos(position * div_term)\n",
                "            self.pe = tf.constant(pe, dtype=tf.float32)\n",
                "\n",
                "        def compute_output_shape(self, input_shape):\n",
                "            return input_shape\n",
                "\n",
                "        def call(self, x):\n",
                "            # x shape: [Batch, Seq, Dim]\n",
                "            seq_len = tf.shape(x)[1]\n",
                "            return x + self.pe[:seq_len, :]\n",
                "\n",
                "    x = PositionalEncoding(EMBED_DIM)(x)\n",
                "    if DROPOUT > 0:\n",
                "        x = layers.Dropout(DROPOUT)(x)\n",
                "\n",
                "    # Transformer Encoder\n",
                "    for _ in range(NUM_LAYERS):\n",
                "        # MultiHead Attention\n",
                "        # key_dim = embed_dim // num_heads\n",
                "        att_output = layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=EMBED_DIM//NUM_HEADS)(x, x)\n",
                "        if DROPOUT > 0:\n",
                "            att_output = layers.Dropout(DROPOUT)(att_output)\n",
                "        x1 = layers.LayerNormalization(epsilon=1e-6)(x + att_output)\n",
                "\n",
                "        # Feed Forward\n",
                "        ffn_output = layers.Dense(FF_DIM, activation='relu')(x1)\n",
                "        ffn_output = layers.Dense(EMBED_DIM)(ffn_output)\n",
                "        if DROPOUT > 0:\n",
                "            ffn_output = layers.Dropout(DROPOUT)(ffn_output)\n",
                "        x = layers.LayerNormalization(epsilon=1e-6)(x1 + ffn_output)\n",
                "\n",
                "    # Decoder Head\n",
                "    # self.decoder0 = nn.Linear(ninp, ninp)\n",
                "    x = layers.Dense(EMBED_DIM, activation='relu')(x)\n",
                "\n",
                "    # self.decoder1 = nn.Linear(ninp, ntoken)\n",
                "    logits = layers.Dense(VOCAB_SIZE)(x)\n",
                "\n",
                "    # Extract center token (Index 8 in sequence of 17)\n",
                "    # octAttention.py: output = output[8, :, :] (Wait, PyTorch is [Seq, Batch, Dim])\n",
                "    # Here we are [Batch, Seq, Dim], so we want [:, 8, :]\n",
                "    center_logits = logits[:, 8, :]\n",
                "\n",
                "    return keras.Model(inputs=inputs, outputs=center_logits)\n",
                "\n",
                "with strategy.scope():\n",
                "    model = create_model()\n",
                "    model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "06c6d973",
            "metadata": {},
            "source": [
                "## 6. Data Pipeline\n",
                "\n",
                "Loads `.fb` files, parses with inlined FlatBuffer classes, and streams to `tf.data`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "21dd7138",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_flatbuffer_data(gcs_file_path_tensor):\n",
                "    \"\"\"Loads a single .fb file from GCS and yields (input, target) pairs.\"\"\"\n",
                "    if hasattr(gcs_file_path_tensor, 'numpy'):\n",
                "        gcs_path = gcs_file_path_tensor.numpy().decode('utf-8')\n",
                "    else:\n",
                "        gcs_path = gcs_file_path_tensor.decode('utf-8')\n",
                "\n",
                "    fs = gcsfs.GCSFileSystem()\n",
                "\n",
                "    temp_file = tempfile.NamedTemporaryFile(suffix='.fb', delete=False)\n",
                "    try:\n",
                "        # Download to local temp\n",
                "        fs.get(gcs_path, temp_file.name)\n",
                "\n",
                "        with open(temp_file.name, 'rb') as f:\n",
                "            buf = f.read()\n",
                "\n",
                "        dataset = Dataset.GetRootAsDataset(buf, 0)\n",
                "        nodes_len = dataset.NodesLength()\n",
                "\n",
                "        # Iterate all nodes\n",
                "        for i in range(nodes_len):\n",
                "            node = dataset.Nodes(i)\n",
                "\n",
                "            # --- Parse Context ---\n",
                "            # Logic from dataset.py: [8 neighbors] + [Parent] + [8 neighbors]\n",
                "            parent_occ = node.ParentOccupancy()\n",
                "            neighbors_len = node.NeighborOccupanciesLength()\n",
                "\n",
                "            # Explicitly loop as per flatbuffers behavior\n",
                "            neighbors = []\n",
                "            for j in range(neighbors_len):\n",
                "                neighbors.append(node.NeighborOccupancies(j))\n",
                "\n",
                "            # Create 17-length context\n",
                "            context = np.zeros(17, dtype=np.int32)\n",
                "            if len(neighbors) >= 16: # Safety check, should be 16\n",
                "                context[0:8] = neighbors[0:8]\n",
                "                context[8] = parent_occ\n",
                "                context[9:] = neighbors[8:16]\n",
                "            else:\n",
                "                # Handle edge case if schema differs\n",
                "                context[8] = parent_occ\n",
                "\n",
                "            # All inputs: [17, 3]\n",
                "            # Channel 0: Occupancy (Context)\n",
                "            # Channel 1: Level (Scalar repeated)\n",
                "            # Channel 2: Octant (Scalar at center, 0 elsewhere)\n",
                "\n",
                "            input_tensor = np.zeros((17, 3), dtype=np.int32)\n",
                "            input_tensor[:, 0] = context\n",
                "\n",
                "            lvl = node.Level()\n",
                "            input_tensor[:, 1] = max(0, lvl - 1) # dataset.py logic\n",
                "\n",
                "            octant = node.Octant()\n",
                "            input_tensor[8, 2] = octant # Only center has octant info\n",
                "\n",
                "            # Target\n",
                "            target = node.Occupancy()\n",
                "\n",
                "            yield input_tensor, target\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading {gcs_path}: {e}\")\n",
                "    finally:\n",
                "        if os.path.exists(temp_file.name):\n",
                "            os.remove(temp_file.name)\n",
                "\n",
                "def create_dataset(file_pattern, batch_size, is_training=True):\n",
                "    files = tf.io.gfile.glob(file_pattern)\n",
                "    print_log(f\"Found {len(files)} files matching {file_pattern}\")\n",
                "    if not files:\n",
                "        raise ValueError(f\"No files found for {file_pattern}\")\n",
                "\n",
                "    dataset = tf.data.Dataset.from_tensor_slices(files)\n",
                "    if is_training:\n",
                "        dataset = dataset.shuffle(len(files))\n",
                "        dataset = dataset.repeat()\n",
                "\n",
                "    dataset = dataset.interleave(\n",
                "        lambda x: tf.data.Dataset.from_generator(\n",
                "            load_flatbuffer_data,\n",
                "            output_signature=(\n",
                "                tf.TensorSpec(shape=(17, 3), dtype=tf.int32),\n",
                "                tf.TensorSpec(shape=(), dtype=tf.int32)\n",
                "            ),\n",
                "            args=[x]\n",
                "        ),\n",
                "        cycle_length=tf.data.AUTOTUNE,\n",
                "        block_length=1,\n",
                "        num_parallel_calls=tf.data.AUTOTUNE\n",
                "    )\n",
                "\n",
                "    if is_training:\n",
                "        dataset = dataset.shuffle(10000)\n",
                "\n",
                "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
                "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
                "    return dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3df3ac3b",
            "metadata": {},
            "source": [
                "## 7. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "815b0ec6",
            "metadata": {},
            "outputs": [],
            "source": [
                "train_ds = create_dataset(GCS_DATA_PATTERN, GLOBAL_BATCH_SIZE)\n",
                "train_dist_ds = strategy.experimental_distribute_dataset(train_ds)\n",
                "\n",
                "with strategy.scope():\n",
                "    optimizer = keras.optimizers.Adam(learning_rate=INITIAL_LR)\n",
                "    loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
                "\n",
                "    def compute_loss(labels, predictions):\n",
                "        per_example_loss = loss_object(labels, predictions)\n",
                "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
                "\n",
                "    @tf.function\n",
                "    def train_step(inputs):\n",
                "        x, y = inputs\n",
                "        with tf.GradientTape() as tape:\n",
                "            predictions = model(x, training=True)\n",
                "            loss = compute_loss(y, predictions)\n",
                "        gradients = tape.gradient(loss, model.trainable_variables)\n",
                "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
                "        return loss\n",
                "\n",
                "# --- Run Training ---\n",
                "print_log(\"Starting Training Loop...\")\n",
                "STEPS_PER_EPOCH = 1000 # Configurable or calculated based on dataset size\n",
                "train_iter = iter(train_dist_ds)\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    print_log(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
                "    total_loss = 0.0\n",
                "    start = time.time()\n",
                "\n",
                "    for step in range(STEPS_PER_EPOCH):\n",
                "        loss = strategy.run(train_step, args=(next(train_iter),))\n",
                "        loss = strategy.reduce(tf.distribute.ReduceOp.SUM, loss, axis=None)\n",
                "        total_loss += loss\n",
                "\n",
                "        if step % 100 == 0:\n",
                "             print(f\"  Step {step}, Loss: {loss:.4f}\")\n",
                "\n",
                "    avg_loss = total_loss / STEPS_PER_EPOCH\n",
                "    duration = time.time() - start\n",
                "    print_log(f\"  Epoch Ended. Avg Loss: {avg_loss:.4f}, Time: {duration:.2f}s\")\n",
                "\n",
                "    # Save Checkpoint\n",
                "    if (epoch + 1) % 5 == 0:\n",
                "        save_path = f\"{GCS_CHECKPOINT_DIR}/model_epoch_{epoch+1}.h5\"\n",
                "        model.save_weights(save_path)\n",
                "        print_log(f\"  Saved checkpoint to {save_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
