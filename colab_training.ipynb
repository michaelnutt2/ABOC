{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ABOC Model Training on Google Colab (TensorFlow + FlatBuffers)\n",
                "\n",
                "This notebook trains the Ancestor-Based Octree Compression (ABOC) model using TensorFlow on TPUs. \n",
                "It loads data from **FlatBuffer (.fb)** files stored in a Google Cloud Storage (GCS) bucket.\n",
                "\n",
                "## Source Code\n",
                "Code is pulled from GitHub: [michaelnutt2/ABOC](https://github.com/michaelnutt2/ABOC)\n",
                "\n",
                "**Key Features:**\n",
                "*   **TPU Acceleration:** Uses `tf.distribute.TPUStrategy` for high-performance training.\n",
                "*   **GCS Integration:** Streams data directly from GCS (`gs://mtn_fb_file_bucket`) to local Colab runtime for processing.\n",
                "*   **FlatBuffers:** Efficiently parses `.fb` files via the `OctreeData` module.\n",
                "*   **Validation Split:** Automatically splits data into Training (90%) and Validation (10%) sets."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Clone Repo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q tensorflow gcsfs flatbuffers\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import time\n",
                "\n",
                "# Clone the repository\n",
                "PROJECT_PATH = '/content/ABOC'\n",
                "if not os.path.exists(PROJECT_PATH):\n",
                "    !git clone https://github.com/michaelnutt2/ABOC.git\n",
                "else:\n",
                "    print(\"Repo already cloned. Pulling latest...\")\n",
                "    !cd {PROJECT_PATH} && git pull\n",
                "\n",
                "os.chdir(PROJECT_PATH)\n",
                "sys.path.append(PROJECT_PATH)\n",
                "print(f\"Working Directory set to: {os.getcwd()}\")\n",
                "\n",
                "import math\n",
                "import glob\n",
                "import tempfile\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "import flatbuffers\n",
                "import gcsfs\n",
                "\n",
                "# Authenticate with Google Cloud\n",
                "from google.colab import auth\n",
                "auth.authenticate_user()\n",
                "print(\"Authenticated with Google Cloud.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration & TPU Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import Config from Repo\n",
                "import config\n",
                "\n",
                "# --- GCS Configuration ---\n",
                "GCS_BUCKET = config.BUCKET_NAME\n",
                "GCS_BASE_URI = f\"gs://{GCS_BUCKET}\"\n",
                "# Pattern to match .fb files. Adjust if files are in a subdir (e.g., 'data/*.fb')\n",
                "GCS_DATA_PATTERN = f\"{GCS_BASE_URI}/{config.GCS_DATA_PREFIX}/*.fb\"\n",
                "GCS_LOG_DIR = f\"{GCS_BASE_URI}/logs\"\n",
                "GCS_CHECKPOINT_DIR = f\"{GCS_BASE_URI}/{config.GCS_CHECKPOINT_PREFIX}\"\n",
                "\n",
                "# --- Model Hyperparameters ---\n",
                "EPOCHS = config.EPOCHS\n",
                "CONTEXT_LEN = config.CONTEXT_LEN\n",
                "MAX_OCTREE_LEVEL = config.MAX_OCTREE_LEVEL\n",
                "VOCAB_SIZE = config.VOCAB_SIZE\n",
                "EMBED_DIM = config.EMBED_DIM\n",
                "NUM_HEADS = config.NUM_HEADS\n",
                "FF_DIM = config.FF_DIM\n",
                "NUM_LAYERS = config.NUM_LAYERS\n",
                "DROPOUT = config.DROPOUT\n",
                "\n",
                "# Learning Rate\n",
                "INITIAL_LR = 1e-3\n",
                "\n",
                "# --- TPU Setup ---\n",
                "try:\n",
                "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
                "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
                "    tf.config.experimental_connect_to_cluster(tpu)\n",
                "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
                "    strategy = tf.distribute.TPUStrategy(tpu)\n",
                "except ValueError:\n",
                "    print('TPU not found. Using Default Strategy (CPU/GPU).')\n",
                "    strategy = tf.distribute.get_strategy()\n",
                "\n",
                "BATCH_SIZE_PER_REPLICA = 128 # Adjust based on memory\n",
                "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
                "print(f\"Global Batch Size: {GLOBAL_BATCH_SIZE}\")\n",
                "print(f\"Data Pattern: {GCS_DATA_PATTERN}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading (Using Repo Modules)\n",
                "\n",
                "We use `OctreeData.Dataset` and `OctreeData.OctreeNode` from the cloned repository to parse FlatBuffers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import OctreeData.Dataset as Dataset\n",
                "import OctreeData.OctreeNode as OctreeNode\n",
                "\n",
                "def parse_flatbuffer_bytes(buf):\n",
                "    \"\"\"\n",
                "    Parses a single FlatBuffer byte string.\n",
                "    Returns: Tuple of arrays (Context, Level, Octant, Label)\n",
                "    \"\"\"\n",
                "    dataset = Dataset.Dataset.GetRootAsDataset(buf, 0)\n",
                "    \n",
                "    num_nodes = dataset.NodesLength()\n",
                "    \n",
                "    # Pre-allocate arrays\n",
                "    # Inputs: [Occupancy_Context, Level, Octant]\n",
                "    # context_len matches config\n",
                "    contexts = np.zeros((num_nodes, CONTEXT_LEN), dtype=np.int32)\n",
                "    levels = np.zeros((num_nodes,), dtype=np.int32)\n",
                "    octants = np.zeros((num_nodes,), dtype=np.int32)\n",
                "    labels = np.zeros((num_nodes,), dtype=np.int32)\n",
                "\n",
                "    mid = CONTEXT_LEN // 2\n",
                "    \n",
                "    for i in range(num_nodes):\n",
                "        node = dataset.Nodes(i)\n",
                "        \n",
                "        # Label\n",
                "        labels[i] = node.Occupancy()\n",
                "        \n",
                "        # Features\n",
                "        levels[i] = max(0, node.Level() - 1)\n",
                "        octants[i] = node.Octant()\n",
                "        \n",
                "        # Context Construction\n",
                "        # Use NeighborOccupanciesAsNumpy if available (Requires updated generated code)\n",
                "        try:\n",
                "            neighbors = node.NeighborOccupanciesAsNumpy()\n",
                "        except AttributeError:\n",
                "            # Fallback if generated code is old\n",
                "            n_len = node.NeighborOccupanciesLength()\n",
                "            neighbors = np.zeros(n_len, dtype=np.int32)\n",
                "            for j in range(n_len):\n",
                "                neighbors[j] = node.NeighborOccupancies(j)\n",
                "        \n",
                "        n_len = len(neighbors)\n",
                "        needed = CONTEXT_LEN - 1\n",
                "        \n",
                "        if n_len >= needed:\n",
                "            contexts[i, 0:mid] = neighbors[0:mid]\n",
                "            contexts[i, mid] = node.ParentOccupancy()\n",
                "            contexts[i, mid+1:] = neighbors[mid:]\n",
                "        else:\n",
                "            # Pad or partial fill (Safe fallback)\n",
                "            contexts[i, mid] = node.ParentOccupancy()\n",
                "            \n",
                "    return contexts, levels, octants, labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare Data Split\n",
                "fs = gcsfs.GCSFileSystem()\n",
                "all_files = fs.glob(GCS_DATA_PATTERN)\n",
                "print(f\"Found {len(all_files)} files in GCS.\")\n",
                "\n",
                "np.random.shuffle(all_files)\n",
                "\n",
                "VAL_SPLIT = 0.1\n",
                "split_idx = int(len(all_files) * (1 - VAL_SPLIT))\n",
                "\n",
                "train_files = all_files[:split_idx]\n",
                "val_files = all_files[split_idx:]\n",
                "\n",
                "print(f\"Training Files: {len(train_files)}\")\n",
                "print(f\"Validation Files: {len(val_files)}\")\n",
                "\n",
                "def data_generator(file_list):\n",
                "    \"\"\"\n",
                "    Generator that yields (inputs, labels) for tf.data.Dataset.\n",
                "    Reads from GCS bucket files provided in file_list.\n",
                "    \"\"\"\n",
                "    # Shuffle files each epoch (or relies on dataset.shuffle)\n",
                "    # If using .from_generator, better to shuffle file list per epoch inside generator if possible, \n",
                "    # OR rely on shuffle buffer (but buffer needs to be large).\n",
                "    # We'll shuffle list here if we loop indefinitely, but tf.data handles epochs.\n",
                "    # We iterate once per 'repeat'.\n",
                "    \n",
                "    random_files = list(file_list)\n",
                "    np.random.shuffle(random_files)\n",
                "    \n",
                "    fs_local = gcsfs.GCSFileSystem()\n",
                "    \n",
                "    for f_path in random_files:\n",
                "        try:\n",
                "            with fs_local.open(f_path, 'rb') as f:\n",
                "                buf = f.read()\n",
                "                \n",
                "            ctx, lvl, octat, y = parse_flatbuffer_bytes(buf)\n",
                "            yield (ctx, lvl, octat), y\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Error processing {f_path}: {e}\")\n",
                "            continue\n",
                "\n",
                "def create_dataset(file_list, batch_size, is_training=True):\n",
                "    \n",
                "    # Wrap generator with args\n",
                "    def gen():\n",
                "        for x in data_generator(file_list):\n",
                "            yield x\n",
                "\n",
                "    ds = tf.data.Dataset.from_generator(\n",
                "        gen,\n",
                "        output_signature=(\n",
                "            (tf.TensorSpec(shape=(None, CONTEXT_LEN), dtype=tf.int32), \n",
                "             tf.TensorSpec(shape=(None,), dtype=tf.int32), \n",
                "             tf.TensorSpec(shape=(None,), dtype=tf.int32)),\n",
                "            tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
                "        )\n",
                "    )\n",
                "    \n",
                "    def unbatch_file(inputs, labels):\n",
                "        ctx, lvl, octat = inputs\n",
                "        ds_file = tf.data.Dataset.from_tensor_slices(((ctx, lvl, octat), labels))\n",
                "        return ds_file\n",
                "        \n",
                "    ds = ds.flat_map(unbatch_file)\n",
                "    \n",
                "    if is_training:\n",
                "        ds = ds.shuffle(buffer_size=100000)\n",
                "    \n",
                "    ds = ds.batch(batch_size, drop_remainder=is_training) # Drop remainder for train (TPU req?), optional for val\n",
                "    \n",
                "    def format_inputs(inputs, label):\n",
                "        ctx, lvl, octat = inputs\n",
                "        \n",
                "        lvl_expanded = tf.expand_dims(lvl, -1) # [Batch, 1]\n",
                "        lvl_seq = tf.repeat(lvl_expanded, CONTEXT_LEN, axis=1) # [Batch, Seq]\n",
                "        \n",
                "        oct_expanded = tf.expand_dims(octat, -1)\n",
                "        oct_seq = tf.repeat(oct_expanded, CONTEXT_LEN, axis=1)\n",
                "        \n",
                "        x = tf.stack([ctx, lvl_seq, oct_seq], axis=-1)\n",
                "        return x, label\n",
                "        \n",
                "    ds = ds.map(format_inputs, num_parallel_calls=tf.data.AUTOTUNE)\n",
                "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
                "    \n",
                "    return ds"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Creation & Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Datasets\n",
                "train_ds = create_dataset(train_files, GLOBAL_BATCH_SIZE, is_training=True)\n",
                "val_ds = create_dataset(val_files, GLOBAL_BATCH_SIZE, is_training=False)\n",
                "\n",
                "# Import model definition from repo\n",
                "from tf_model import create_model\n",
                "\n",
                "with strategy.scope():\n",
                "    model = create_model()\n",
                "    optimizer = keras.optimizers.Adam(learning_rate=INITIAL_LR)\n",
                "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
                "    \n",
                "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
                "    model.summary()\n",
                "\n",
                "# Checkpoint Callback\n",
                "checkpoint_path = f\"{GCS_CHECKPOINT_DIR}/checkpoints_model_epoch_{{epoch:02d}}.weights.h5\"\n",
                "cp_callback = keras.callbacks.ModelCheckpoint(\n",
                "    filepath=checkpoint_path,\n",
                "    save_weights_only=True,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# TensorBoard\n",
                "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=GCS_LOG_DIR, histogram_freq=1)\n",
                "\n",
                "# Train\n",
                "history = model.fit(\n",
                "    train_ds,\n",
                "    validation_data=val_ds,\n",
                "    epochs=EPOCHS,\n",
                "    steps_per_epoch=200, # Approx steps per epoch (since generator is infinite/unbatched)\n",
                "    validation_steps=20, # Steps to validate\n",
                "    callbacks=[cp_callback, tensorboard_callback]\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}